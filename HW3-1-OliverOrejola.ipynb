{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bbb2fc",
   "metadata": {},
   "source": [
    "# HW3-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4628830",
   "metadata": {},
   "source": [
    "Import relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28327ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfc102",
   "metadata": {},
   "source": [
    "Generate 1000 samples from two classes of multivaraite normals as well as their associated labels positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b7388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = 1000\n",
    "\n",
    "negative_samples = np.random.multivariate_normal(mean=[0,3],cov=[[1,0.5],[0.5,1]],size = num_samples_per_class)\n",
    "positive_samples = np.random.multivariate_normal(mean=[3,0],cov=[[1,0.5],[0.5,1]],size = num_samples_per_class)\n",
    "\n",
    "inputs = np.vstack((negative_samples,positive_samples)).astype(np.float32)\n",
    "\n",
    "targets = np.vstack((np.zeros((num_samples_per_class,1),dtype=\"float32\"),np.ones((num_samples_per_class,1),dtype=\"float32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2a5c4",
   "metadata": {},
   "source": [
    "**Define Model**\n",
    "* Layer variables\n",
    "* ReLu\n",
    "* Forward pass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "297f286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "hidden_dim = 5\n",
    "output_dim = 1\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(input_dim,hidden_dim)))\n",
    "b1 = tf.Variable(tf.random.uniform(shape=(hidden_dim,)))\n",
    "\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim,output_dim)))\n",
    "b2 = tf.Variable(tf.random.uniform(shape=(output_dim,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91425322",
   "metadata": {},
   "source": [
    "**Homework Part** define a TensorFlow ReLu\n",
    "\n",
    "$\n",
    "Relu(x) = max(x,0)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9f8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(x):\n",
    "    return(tf.math.maximum(0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21bc38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    hidden = ReLu(tf.matmul(inputs,W1)+b1)\n",
    "    return tf.matmul(hidden,W2)+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a66d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(targets,predictions):\n",
    "    per_sample_losses = tf.square(targets-predictions) \n",
    "    return tf.reduce_mean(per_sample_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8f9d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(inputs,targets,learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = square_loss(predictions,targets)\n",
    "    grad_loss_wrt_W1, grad_loss_wrt_b1 ,grad_loss_wrt_W2, grad_loss_wrt_b2  = tape.gradient(loss,[W1,b1,W2,b2])\n",
    "    W1.assign_sub(grad_loss_wrt_W1 * learning_rate)\n",
    "    b1.assign_sub(grad_loss_wrt_b1* learning_rate)\n",
    "    W2.assign_sub(grad_loss_wrt_W2 * learning_rate)\n",
    "    b2.assign_sub(grad_loss_wrt_b2* learning_rate)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a35dc",
   "metadata": {},
   "source": [
    "Learning rate `0.05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1189a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 76.358231\n",
      "loss at step 1: 3.896795\n",
      "loss at step 2: 1.301066\n",
      "loss at step 3: 0.840102\n",
      "loss at step 4: 0.696481\n",
      "loss at step 5: 0.614691\n",
      "loss at step 6: 0.557292\n",
      "loss at step 7: 0.514452\n",
      "loss at step 8: 0.480402\n",
      "loss at step 9: 0.453352\n",
      "loss at step 10: 0.431234\n",
      "loss at step 11: 0.412955\n",
      "loss at step 12: 0.397528\n",
      "loss at step 13: 0.384329\n",
      "loss at step 14: 0.373414\n",
      "loss at step 15: 0.364038\n",
      "loss at step 16: 0.355761\n",
      "loss at step 17: 0.348426\n",
      "loss at step 18: 0.341864\n",
      "loss at step 19: 0.336091\n",
      "loss at step 20: 0.330972\n",
      "loss at step 21: 0.326297\n",
      "loss at step 22: 0.322103\n",
      "loss at step 23: 0.318303\n",
      "loss at step 24: 0.314856\n",
      "loss at step 25: 0.311702\n",
      "loss at step 26: 0.308819\n",
      "loss at step 27: 0.306173\n",
      "loss at step 28: 0.303716\n",
      "loss at step 29: 0.301443\n",
      "loss at step 30: 0.299312\n",
      "loss at step 31: 0.297280\n",
      "loss at step 32: 0.295343\n",
      "loss at step 33: 0.293494\n",
      "loss at step 34: 0.291731\n",
      "loss at step 35: 0.289956\n",
      "loss at step 36: 0.288193\n",
      "loss at step 37: 0.286461\n",
      "loss at step 38: 0.284665\n",
      "loss at step 39: 0.282702\n",
      "loss at step 40: 0.280533\n",
      "loss at step 41: 0.278077\n",
      "loss at step 42: 0.275020\n",
      "loss at step 43: 0.270238\n",
      "loss at step 44: 0.262456\n",
      "loss at step 45: 0.248799\n",
      "loss at step 46: 0.221006\n",
      "loss at step 47: 0.184267\n",
      "loss at step 48: 0.163246\n",
      "loss at step 49: 0.144993\n",
      "loss at step 50: 0.130383\n",
      "loss at step 51: 0.118757\n",
      "loss at step 52: 0.109424\n",
      "loss at step 53: 0.101760\n",
      "loss at step 54: 0.095408\n",
      "loss at step 55: 0.090061\n",
      "loss at step 56: 0.085463\n",
      "loss at step 57: 0.081474\n",
      "loss at step 58: 0.077962\n",
      "loss at step 59: 0.074825\n",
      "loss at step 60: 0.071992\n",
      "loss at step 61: 0.069416\n",
      "loss at step 62: 0.067065\n",
      "loss at step 63: 0.064934\n",
      "loss at step 64: 0.063027\n",
      "loss at step 65: 0.061268\n",
      "loss at step 66: 0.059714\n",
      "loss at step 67: 0.058316\n",
      "loss at step 68: 0.057106\n",
      "loss at step 69: 0.055985\n",
      "loss at step 70: 0.055089\n",
      "loss at step 71: 0.054355\n",
      "loss at step 72: 0.053773\n",
      "loss at step 73: 0.053352\n",
      "loss at step 74: 0.053081\n",
      "loss at step 75: 0.053110\n",
      "loss at step 76: 0.053246\n",
      "loss at step 77: 0.053802\n",
      "loss at step 78: 0.054335\n",
      "loss at step 79: 0.055664\n",
      "loss at step 80: 0.056603\n",
      "loss at step 81: 0.058738\n",
      "loss at step 82: 0.060033\n",
      "loss at step 83: 0.063421\n",
      "loss at step 84: 0.064936\n",
      "loss at step 85: 0.069446\n",
      "loss at step 86: 0.070974\n",
      "loss at step 87: 0.076652\n",
      "loss at step 88: 0.077672\n",
      "loss at step 89: 0.084172\n",
      "loss at step 90: 0.084249\n",
      "loss at step 91: 0.090987\n",
      "loss at step 92: 0.089605\n",
      "loss at step 93: 0.095934\n",
      "loss at step 94: 0.092891\n",
      "loss at step 95: 0.098214\n",
      "loss at step 96: 0.093424\n",
      "loss at step 97: 0.097337\n",
      "loss at step 98: 0.091107\n",
      "loss at step 99: 0.093504\n",
      "loss at step 100: 0.086433\n",
      "loss at step 101: 0.087458\n",
      "loss at step 102: 0.080196\n",
      "loss at step 103: 0.080177\n",
      "loss at step 104: 0.073333\n",
      "loss at step 105: 0.072647\n",
      "loss at step 106: 0.066579\n",
      "loss at step 107: 0.065582\n",
      "loss at step 108: 0.060440\n",
      "loss at step 109: 0.059354\n",
      "loss at step 110: 0.055085\n",
      "loss at step 111: 0.053979\n",
      "loss at step 112: 0.050550\n",
      "loss at step 113: 0.049498\n",
      "loss at step 114: 0.046745\n",
      "loss at step 115: 0.045791\n",
      "loss at step 116: 0.043565\n",
      "loss at step 117: 0.042725\n",
      "loss at step 118: 0.040907\n",
      "loss at step 119: 0.040173\n",
      "loss at step 120: 0.038717\n",
      "loss at step 121: 0.038079\n",
      "loss at step 122: 0.036902\n",
      "loss at step 123: 0.036351\n",
      "loss at step 124: 0.035391\n",
      "loss at step 125: 0.034905\n",
      "loss at step 126: 0.034090\n",
      "loss at step 127: 0.033669\n",
      "loss at step 128: 0.032990\n",
      "loss at step 129: 0.032618\n",
      "loss at step 130: 0.032057\n",
      "loss at step 131: 0.031727\n",
      "loss at step 132: 0.031247\n",
      "loss at step 133: 0.030957\n",
      "loss at step 134: 0.030551\n",
      "loss at step 135: 0.030297\n",
      "loss at step 136: 0.029955\n",
      "loss at step 137: 0.029728\n",
      "loss at step 138: 0.029435\n",
      "loss at step 139: 0.029230\n",
      "loss at step 140: 0.028969\n",
      "loss at step 141: 0.028782\n",
      "loss at step 142: 0.028549\n",
      "loss at step 143: 0.028382\n",
      "loss at step 144: 0.028178\n",
      "loss at step 145: 0.028024\n",
      "loss at step 146: 0.027841\n",
      "loss at step 147: 0.027698\n",
      "loss at step 148: 0.027536\n",
      "loss at step 149: 0.027404\n",
      "loss at step 150: 0.027258\n",
      "loss at step 151: 0.027138\n",
      "loss at step 152: 0.027004\n",
      "loss at step 153: 0.026893\n",
      "loss at step 154: 0.026769\n",
      "loss at step 155: 0.026664\n",
      "loss at step 156: 0.026550\n",
      "loss at step 157: 0.026452\n",
      "loss at step 158: 0.026348\n",
      "loss at step 159: 0.026258\n",
      "loss at step 160: 0.026162\n",
      "loss at step 161: 0.026076\n",
      "loss at step 162: 0.025987\n",
      "loss at step 163: 0.025907\n",
      "loss at step 164: 0.025824\n",
      "loss at step 165: 0.025748\n",
      "loss at step 166: 0.025670\n",
      "loss at step 167: 0.025599\n",
      "loss at step 168: 0.025526\n",
      "loss at step 169: 0.025458\n",
      "loss at step 170: 0.025390\n",
      "loss at step 171: 0.025326\n",
      "loss at step 172: 0.025261\n",
      "loss at step 173: 0.025200\n",
      "loss at step 174: 0.025139\n",
      "loss at step 175: 0.025080\n",
      "loss at step 176: 0.025021\n",
      "loss at step 177: 0.024964\n",
      "loss at step 178: 0.024908\n",
      "loss at step 179: 0.024854\n",
      "loss at step 180: 0.024800\n",
      "loss at step 181: 0.024748\n",
      "loss at step 182: 0.024696\n",
      "loss at step 183: 0.024645\n",
      "loss at step 184: 0.024595\n",
      "loss at step 185: 0.024546\n",
      "loss at step 186: 0.024497\n",
      "loss at step 187: 0.024450\n",
      "loss at step 188: 0.024403\n",
      "loss at step 189: 0.024357\n",
      "loss at step 190: 0.024312\n",
      "loss at step 191: 0.024269\n",
      "loss at step 192: 0.024225\n",
      "loss at step 193: 0.024183\n",
      "loss at step 194: 0.024142\n",
      "loss at step 195: 0.024101\n",
      "loss at step 196: 0.024062\n",
      "loss at step 197: 0.024023\n",
      "loss at step 198: 0.023984\n",
      "loss at step 199: 0.023947\n",
      "loss at step 200: 0.023911\n",
      "loss at step 201: 0.023875\n",
      "loss at step 202: 0.023840\n",
      "loss at step 203: 0.023805\n",
      "loss at step 204: 0.023771\n",
      "loss at step 205: 0.023738\n",
      "loss at step 206: 0.023705\n",
      "loss at step 207: 0.023672\n",
      "loss at step 208: 0.023641\n",
      "loss at step 209: 0.023610\n",
      "loss at step 210: 0.023579\n",
      "loss at step 211: 0.023549\n",
      "loss at step 212: 0.023519\n",
      "loss at step 213: 0.023490\n",
      "loss at step 214: 0.023461\n",
      "loss at step 215: 0.023432\n",
      "loss at step 216: 0.023404\n",
      "loss at step 217: 0.023375\n",
      "loss at step 218: 0.023348\n",
      "loss at step 219: 0.023320\n",
      "loss at step 220: 0.023293\n",
      "loss at step 221: 0.023267\n",
      "loss at step 222: 0.023241\n",
      "loss at step 223: 0.023215\n",
      "loss at step 224: 0.023189\n",
      "loss at step 225: 0.023164\n",
      "loss at step 226: 0.023139\n",
      "loss at step 227: 0.023115\n",
      "loss at step 228: 0.023090\n",
      "loss at step 229: 0.023067\n",
      "loss at step 230: 0.023049\n",
      "loss at step 231: 0.023027\n",
      "loss at step 232: 0.023009\n",
      "loss at step 233: 0.022990\n",
      "loss at step 234: 0.022972\n",
      "loss at step 235: 0.022954\n",
      "loss at step 236: 0.022937\n",
      "loss at step 237: 0.022920\n",
      "loss at step 238: 0.022902\n",
      "loss at step 239: 0.022888\n",
      "loss at step 240: 0.022869\n",
      "loss at step 241: 0.022856\n",
      "loss at step 242: 0.022838\n",
      "loss at step 243: 0.022826\n",
      "loss at step 244: 0.022807\n",
      "loss at step 245: 0.022796\n",
      "loss at step 246: 0.022776\n",
      "loss at step 247: 0.022765\n",
      "loss at step 248: 0.022745\n",
      "loss at step 249: 0.022736\n",
      "loss at step 250: 0.022714\n",
      "loss at step 251: 0.022706\n",
      "loss at step 252: 0.022684\n",
      "loss at step 253: 0.022677\n",
      "loss at step 254: 0.022655\n",
      "loss at step 255: 0.022649\n",
      "loss at step 256: 0.022626\n",
      "loss at step 257: 0.022621\n",
      "loss at step 258: 0.022597\n",
      "loss at step 259: 0.022593\n",
      "loss at step 260: 0.022568\n",
      "loss at step 261: 0.022565\n",
      "loss at step 262: 0.022540\n",
      "loss at step 263: 0.022538\n",
      "loss at step 264: 0.022513\n",
      "loss at step 265: 0.022490\n",
      "loss at step 266: 0.022466\n",
      "loss at step 267: 0.022445\n",
      "loss at step 268: 0.022422\n",
      "loss at step 269: 0.022401\n",
      "loss at step 270: 0.022379\n",
      "loss at step 271: 0.022359\n",
      "loss at step 272: 0.022337\n",
      "loss at step 273: 0.022318\n",
      "loss at step 274: 0.022297\n",
      "loss at step 275: 0.022279\n",
      "loss at step 276: 0.022259\n",
      "loss at step 277: 0.022241\n",
      "loss at step 278: 0.022222\n",
      "loss at step 279: 0.022204\n",
      "loss at step 280: 0.022186\n",
      "loss at step 281: 0.022169\n",
      "loss at step 282: 0.022152\n",
      "loss at step 283: 0.022135\n",
      "loss at step 284: 0.022118\n",
      "loss at step 285: 0.022101\n",
      "loss at step 286: 0.022084\n",
      "loss at step 287: 0.022067\n",
      "loss at step 288: 0.022051\n",
      "loss at step 289: 0.022035\n",
      "loss at step 290: 0.022018\n",
      "loss at step 291: 0.022003\n",
      "loss at step 292: 0.021987\n",
      "loss at step 293: 0.021972\n",
      "loss at step 294: 0.021957\n",
      "loss at step 295: 0.021942\n",
      "loss at step 296: 0.021927\n",
      "loss at step 297: 0.021913\n",
      "loss at step 298: 0.021899\n",
      "loss at step 299: 0.021886\n",
      "loss at step 300: 0.021873\n",
      "loss at step 301: 0.021860\n",
      "loss at step 302: 0.021847\n",
      "loss at step 303: 0.021835\n",
      "loss at step 304: 0.021822\n",
      "loss at step 305: 0.021810\n",
      "loss at step 306: 0.021798\n",
      "loss at step 307: 0.021786\n",
      "loss at step 308: 0.021774\n",
      "loss at step 309: 0.021763\n",
      "loss at step 310: 0.021751\n",
      "loss at step 311: 0.021740\n",
      "loss at step 312: 0.021729\n",
      "loss at step 313: 0.021718\n",
      "loss at step 314: 0.021706\n",
      "loss at step 315: 0.021696\n",
      "loss at step 316: 0.021685\n",
      "loss at step 317: 0.021674\n",
      "loss at step 318: 0.021663\n",
      "loss at step 319: 0.021653\n",
      "loss at step 320: 0.021643\n",
      "loss at step 321: 0.021632\n",
      "loss at step 322: 0.021622\n",
      "loss at step 323: 0.021612\n",
      "loss at step 324: 0.021602\n",
      "loss at step 325: 0.021592\n",
      "loss at step 326: 0.021582\n",
      "loss at step 327: 0.021573\n",
      "loss at step 328: 0.021563\n",
      "loss at step 329: 0.021553\n",
      "loss at step 330: 0.021544\n",
      "loss at step 331: 0.021535\n",
      "loss at step 332: 0.021525\n",
      "loss at step 333: 0.021516\n",
      "loss at step 334: 0.021507\n",
      "loss at step 335: 0.021498\n",
      "loss at step 336: 0.021489\n",
      "loss at step 337: 0.021480\n",
      "loss at step 338: 0.021471\n",
      "loss at step 339: 0.021463\n",
      "loss at step 340: 0.021454\n",
      "loss at step 341: 0.021445\n",
      "loss at step 342: 0.021437\n",
      "loss at step 343: 0.021429\n",
      "loss at step 344: 0.021420\n",
      "loss at step 345: 0.021412\n",
      "loss at step 346: 0.021404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 347: 0.021396\n",
      "loss at step 348: 0.021388\n",
      "loss at step 349: 0.021380\n",
      "loss at step 350: 0.021372\n",
      "loss at step 351: 0.021364\n",
      "loss at step 352: 0.021357\n",
      "loss at step 353: 0.021349\n",
      "loss at step 354: 0.021342\n",
      "loss at step 355: 0.021334\n",
      "loss at step 356: 0.021327\n",
      "loss at step 357: 0.021320\n",
      "loss at step 358: 0.021312\n",
      "loss at step 359: 0.021305\n",
      "loss at step 360: 0.021298\n",
      "loss at step 361: 0.021291\n",
      "loss at step 362: 0.021284\n",
      "loss at step 363: 0.021277\n",
      "loss at step 364: 0.021270\n",
      "loss at step 365: 0.021264\n",
      "loss at step 366: 0.021257\n",
      "loss at step 367: 0.021250\n",
      "loss at step 368: 0.021244\n",
      "loss at step 369: 0.021237\n",
      "loss at step 370: 0.021230\n",
      "loss at step 371: 0.021224\n",
      "loss at step 372: 0.021218\n",
      "loss at step 373: 0.021211\n",
      "loss at step 374: 0.021205\n",
      "loss at step 375: 0.021199\n",
      "loss at step 376: 0.021192\n",
      "loss at step 377: 0.021186\n",
      "loss at step 378: 0.021180\n",
      "loss at step 379: 0.021174\n",
      "loss at step 380: 0.021168\n",
      "loss at step 381: 0.021162\n",
      "loss at step 382: 0.021156\n",
      "loss at step 383: 0.021150\n",
      "loss at step 384: 0.021144\n",
      "loss at step 385: 0.021138\n",
      "loss at step 386: 0.021133\n",
      "loss at step 387: 0.021127\n",
      "loss at step 388: 0.021121\n",
      "loss at step 389: 0.021115\n",
      "loss at step 390: 0.021110\n",
      "loss at step 391: 0.021104\n",
      "loss at step 392: 0.021098\n",
      "loss at step 393: 0.021093\n",
      "loss at step 394: 0.021087\n",
      "loss at step 395: 0.021082\n",
      "loss at step 396: 0.021076\n",
      "loss at step 397: 0.021071\n",
      "loss at step 398: 0.021066\n",
      "loss at step 399: 0.021060\n"
     ]
    }
   ],
   "source": [
    "# Reset parameters of model\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(input_dim,hidden_dim)))\n",
    "b1 = tf.Variable(tf.random.uniform(shape=(hidden_dim,)))\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim,output_dim)))\n",
    "b2 = tf.Variable(tf.random.uniform(shape=(output_dim,)))\n",
    "\n",
    "losses = [] #Initialize blank list to store loss values\n",
    "learning_rate = 0.05\n",
    "\n",
    "for step in range(400):\n",
    "    loss = training_step(inputs,targets,learning_rate)\n",
    "    losses.append(loss)\n",
    "    print(f\"loss at step {step}: {loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55f24c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(400),losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc6164",
   "metadata": {},
   "source": [
    "Learning rate `0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d58aeefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 76.358231\n",
      "loss at step 1: 19.171474\n",
      "loss at step 2: 6.603267\n",
      "loss at step 3: 2.025781\n",
      "loss at step 4: 1.414198\n",
      "loss at step 5: 1.071422\n",
      "loss at step 6: 0.868048\n",
      "loss at step 7: 0.732934\n",
      "loss at step 8: 0.635990\n",
      "loss at step 9: 0.567993\n",
      "loss at step 10: 0.515927\n",
      "loss at step 11: 0.477519\n",
      "loss at step 12: 0.446541\n",
      "loss at step 13: 0.421026\n",
      "loss at step 14: 0.399736\n",
      "loss at step 15: 0.381685\n",
      "loss at step 16: 0.367050\n",
      "loss at step 17: 0.355280\n",
      "loss at step 18: 0.345980\n",
      "loss at step 19: 0.338492\n",
      "loss at step 20: 0.332020\n",
      "loss at step 21: 0.326286\n",
      "loss at step 22: 0.321142\n",
      "loss at step 23: 0.316481\n",
      "loss at step 24: 0.312230\n",
      "loss at step 25: 0.308380\n",
      "loss at step 26: 0.304944\n",
      "loss at step 27: 0.301796\n",
      "loss at step 28: 0.298976\n",
      "loss at step 29: 0.296366\n",
      "loss at step 30: 0.293961\n",
      "loss at step 31: 0.291714\n",
      "loss at step 32: 0.289595\n",
      "loss at step 33: 0.287594\n",
      "loss at step 34: 0.285727\n",
      "loss at step 35: 0.284043\n",
      "loss at step 36: 0.282444\n",
      "loss at step 37: 0.280945\n",
      "loss at step 38: 0.279539\n",
      "loss at step 39: 0.278217\n",
      "loss at step 40: 0.276963\n",
      "loss at step 41: 0.275782\n",
      "loss at step 42: 0.274676\n",
      "loss at step 43: 0.273627\n",
      "loss at step 44: 0.272636\n",
      "loss at step 45: 0.271709\n",
      "loss at step 46: 0.270829\n",
      "loss at step 47: 0.269995\n",
      "loss at step 48: 0.269202\n",
      "loss at step 49: 0.268450\n",
      "loss at step 50: 0.267736\n",
      "loss at step 51: 0.267057\n",
      "loss at step 52: 0.266412\n",
      "loss at step 53: 0.265800\n",
      "loss at step 54: 0.265218\n",
      "loss at step 55: 0.264665\n",
      "loss at step 56: 0.264145\n",
      "loss at step 57: 0.263656\n",
      "loss at step 58: 0.263184\n",
      "loss at step 59: 0.262729\n",
      "loss at step 60: 0.262292\n",
      "loss at step 61: 0.261871\n",
      "loss at step 62: 0.261465\n",
      "loss at step 63: 0.261074\n",
      "loss at step 64: 0.260695\n",
      "loss at step 65: 0.260319\n",
      "loss at step 66: 0.259956\n",
      "loss at step 67: 0.259607\n",
      "loss at step 68: 0.259270\n",
      "loss at step 69: 0.258946\n",
      "loss at step 70: 0.258633\n",
      "loss at step 71: 0.258331\n",
      "loss at step 72: 0.258040\n",
      "loss at step 73: 0.257759\n",
      "loss at step 74: 0.257488\n",
      "loss at step 75: 0.257227\n",
      "loss at step 76: 0.256975\n",
      "loss at step 77: 0.256732\n",
      "loss at step 78: 0.256498\n",
      "loss at step 79: 0.256272\n",
      "loss at step 80: 0.256054\n",
      "loss at step 81: 0.255844\n",
      "loss at step 82: 0.255641\n",
      "loss at step 83: 0.255445\n",
      "loss at step 84: 0.255257\n",
      "loss at step 85: 0.255075\n",
      "loss at step 86: 0.254899\n",
      "loss at step 87: 0.254729\n",
      "loss at step 88: 0.254566\n",
      "loss at step 89: 0.254408\n",
      "loss at step 90: 0.254256\n",
      "loss at step 91: 0.254109\n",
      "loss at step 92: 0.253967\n",
      "loss at step 93: 0.253830\n",
      "loss at step 94: 0.253698\n",
      "loss at step 95: 0.253570\n",
      "loss at step 96: 0.253447\n",
      "loss at step 97: 0.253329\n",
      "loss at step 98: 0.253214\n",
      "loss at step 99: 0.253103\n",
      "loss at step 100: 0.252996\n",
      "loss at step 101: 0.252893\n",
      "loss at step 102: 0.252794\n",
      "loss at step 103: 0.252698\n",
      "loss at step 104: 0.252605\n",
      "loss at step 105: 0.252515\n",
      "loss at step 106: 0.252429\n",
      "loss at step 107: 0.252344\n",
      "loss at step 108: 0.252257\n",
      "loss at step 109: 0.252173\n",
      "loss at step 110: 0.252091\n",
      "loss at step 111: 0.252013\n",
      "loss at step 112: 0.251937\n",
      "loss at step 113: 0.251863\n",
      "loss at step 114: 0.251792\n",
      "loss at step 115: 0.251724\n",
      "loss at step 116: 0.251658\n",
      "loss at step 117: 0.251594\n",
      "loss at step 118: 0.251532\n",
      "loss at step 119: 0.251472\n",
      "loss at step 120: 0.251414\n",
      "loss at step 121: 0.251359\n",
      "loss at step 122: 0.251305\n",
      "loss at step 123: 0.251253\n",
      "loss at step 124: 0.251203\n",
      "loss at step 125: 0.251154\n",
      "loss at step 126: 0.251107\n",
      "loss at step 127: 0.251062\n",
      "loss at step 128: 0.251018\n",
      "loss at step 129: 0.250987\n",
      "loss at step 130: 0.250961\n",
      "loss at step 131: 0.250936\n",
      "loss at step 132: 0.250912\n",
      "loss at step 133: 0.250888\n",
      "loss at step 134: 0.250865\n",
      "loss at step 135: 0.250842\n",
      "loss at step 136: 0.250820\n",
      "loss at step 137: 0.250799\n",
      "loss at step 138: 0.250778\n",
      "loss at step 139: 0.250758\n",
      "loss at step 140: 0.250738\n",
      "loss at step 141: 0.250719\n",
      "loss at step 142: 0.250700\n",
      "loss at step 143: 0.250682\n",
      "loss at step 144: 0.250664\n",
      "loss at step 145: 0.250647\n",
      "loss at step 146: 0.250630\n",
      "loss at step 147: 0.250613\n",
      "loss at step 148: 0.250597\n",
      "loss at step 149: 0.250582\n",
      "loss at step 150: 0.250566\n",
      "loss at step 151: 0.250552\n",
      "loss at step 152: 0.250537\n",
      "loss at step 153: 0.250523\n",
      "loss at step 154: 0.250510\n",
      "loss at step 155: 0.250496\n",
      "loss at step 156: 0.250483\n",
      "loss at step 157: 0.250470\n",
      "loss at step 158: 0.250458\n",
      "loss at step 159: 0.250446\n",
      "loss at step 160: 0.250434\n",
      "loss at step 161: 0.250422\n",
      "loss at step 162: 0.250411\n",
      "loss at step 163: 0.250400\n",
      "loss at step 164: 0.250389\n",
      "loss at step 165: 0.250379\n",
      "loss at step 166: 0.250369\n",
      "loss at step 167: 0.250359\n",
      "loss at step 168: 0.250349\n",
      "loss at step 169: 0.250340\n",
      "loss at step 170: 0.250330\n",
      "loss at step 171: 0.250321\n",
      "loss at step 172: 0.250312\n",
      "loss at step 173: 0.250304\n",
      "loss at step 174: 0.250295\n",
      "loss at step 175: 0.250287\n",
      "loss at step 176: 0.250279\n",
      "loss at step 177: 0.250271\n",
      "loss at step 178: 0.250264\n",
      "loss at step 179: 0.250256\n",
      "loss at step 180: 0.250249\n",
      "loss at step 181: 0.250242\n",
      "loss at step 182: 0.250235\n",
      "loss at step 183: 0.250228\n",
      "loss at step 184: 0.250221\n",
      "loss at step 185: 0.250215\n",
      "loss at step 186: 0.250208\n",
      "loss at step 187: 0.250202\n",
      "loss at step 188: 0.250196\n",
      "loss at step 189: 0.250190\n",
      "loss at step 190: 0.250184\n",
      "loss at step 191: 0.250178\n",
      "loss at step 192: 0.250172\n",
      "loss at step 193: 0.250167\n",
      "loss at step 194: 0.250162\n",
      "loss at step 195: 0.250156\n",
      "loss at step 196: 0.250151\n",
      "loss at step 197: 0.250146\n",
      "loss at step 198: 0.250141\n",
      "loss at step 199: 0.250136\n",
      "loss at step 200: 0.250131\n",
      "loss at step 201: 0.250127\n",
      "loss at step 202: 0.250122\n",
      "loss at step 203: 0.250118\n",
      "loss at step 204: 0.250114\n",
      "loss at step 205: 0.250110\n",
      "loss at step 206: 0.250107\n",
      "loss at step 207: 0.250104\n",
      "loss at step 208: 0.250101\n",
      "loss at step 209: 0.250098\n",
      "loss at step 210: 0.250095\n",
      "loss at step 211: 0.250091\n",
      "loss at step 212: 0.250088\n",
      "loss at step 213: 0.250085\n",
      "loss at step 214: 0.250082\n",
      "loss at step 215: 0.250080\n",
      "loss at step 216: 0.250077\n",
      "loss at step 217: 0.250074\n",
      "loss at step 218: 0.250071\n",
      "loss at step 219: 0.250068\n",
      "loss at step 220: 0.250065\n",
      "loss at step 221: 0.250063\n",
      "loss at step 222: 0.250060\n",
      "loss at step 223: 0.250057\n",
      "loss at step 224: 0.250054\n",
      "loss at step 225: 0.250052\n",
      "loss at step 226: 0.250049\n",
      "loss at step 227: 0.250046\n",
      "loss at step 228: 0.250044\n",
      "loss at step 229: 0.250041\n",
      "loss at step 230: 0.250039\n",
      "loss at step 231: 0.250036\n",
      "loss at step 232: 0.250034\n",
      "loss at step 233: 0.250031\n",
      "loss at step 234: 0.250029\n",
      "loss at step 235: 0.250026\n",
      "loss at step 236: 0.250024\n",
      "loss at step 237: 0.250021\n",
      "loss at step 238: 0.250019\n",
      "loss at step 239: 0.250017\n",
      "loss at step 240: 0.250014\n",
      "loss at step 241: 0.250012\n",
      "loss at step 242: 0.250010\n",
      "loss at step 243: 0.250008\n",
      "loss at step 244: 0.250005\n",
      "loss at step 245: 0.250003\n",
      "loss at step 246: 0.250001\n",
      "loss at step 247: 0.249999\n",
      "loss at step 248: 0.249997\n",
      "loss at step 249: 0.249995\n",
      "loss at step 250: 0.249992\n",
      "loss at step 251: 0.249990\n",
      "loss at step 252: 0.249988\n",
      "loss at step 253: 0.249986\n",
      "loss at step 254: 0.249984\n",
      "loss at step 255: 0.249982\n",
      "loss at step 256: 0.249980\n",
      "loss at step 257: 0.249978\n",
      "loss at step 258: 0.249976\n",
      "loss at step 259: 0.249974\n",
      "loss at step 260: 0.249972\n",
      "loss at step 261: 0.249970\n",
      "loss at step 262: 0.249969\n",
      "loss at step 263: 0.249967\n",
      "loss at step 264: 0.249965\n",
      "loss at step 265: 0.249963\n",
      "loss at step 266: 0.249961\n",
      "loss at step 267: 0.249959\n",
      "loss at step 268: 0.249958\n",
      "loss at step 269: 0.249956\n",
      "loss at step 270: 0.249954\n",
      "loss at step 271: 0.249952\n",
      "loss at step 272: 0.249951\n",
      "loss at step 273: 0.249949\n",
      "loss at step 274: 0.249947\n",
      "loss at step 275: 0.249945\n",
      "loss at step 276: 0.249944\n",
      "loss at step 277: 0.249942\n",
      "loss at step 278: 0.249941\n",
      "loss at step 279: 0.249939\n",
      "loss at step 280: 0.249937\n",
      "loss at step 281: 0.249936\n",
      "loss at step 282: 0.249934\n",
      "loss at step 283: 0.249933\n",
      "loss at step 284: 0.249931\n",
      "loss at step 285: 0.249930\n",
      "loss at step 286: 0.249928\n",
      "loss at step 287: 0.249927\n",
      "loss at step 288: 0.249925\n",
      "loss at step 289: 0.249924\n",
      "loss at step 290: 0.249922\n",
      "loss at step 291: 0.249921\n",
      "loss at step 292: 0.249919\n",
      "loss at step 293: 0.249918\n",
      "loss at step 294: 0.249917\n",
      "loss at step 295: 0.249915\n",
      "loss at step 296: 0.249914\n",
      "loss at step 297: 0.249912\n",
      "loss at step 298: 0.249911\n",
      "loss at step 299: 0.249910\n",
      "loss at step 300: 0.249908\n",
      "loss at step 301: 0.249907\n",
      "loss at step 302: 0.249906\n",
      "loss at step 303: 0.249904\n",
      "loss at step 304: 0.249903\n",
      "loss at step 305: 0.249902\n",
      "loss at step 306: 0.249901\n",
      "loss at step 307: 0.249900\n",
      "loss at step 308: 0.249898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 309: 0.249897\n",
      "loss at step 310: 0.249896\n",
      "loss at step 311: 0.249895\n",
      "loss at step 312: 0.249893\n",
      "loss at step 313: 0.249892\n",
      "loss at step 314: 0.249891\n",
      "loss at step 315: 0.249890\n",
      "loss at step 316: 0.249889\n",
      "loss at step 317: 0.249888\n",
      "loss at step 318: 0.249886\n",
      "loss at step 319: 0.249885\n",
      "loss at step 320: 0.249884\n",
      "loss at step 321: 0.249883\n",
      "loss at step 322: 0.249883\n",
      "loss at step 323: 0.249883\n",
      "loss at step 324: 0.249883\n",
      "loss at step 325: 0.249883\n",
      "loss at step 326: 0.249883\n",
      "loss at step 327: 0.249883\n",
      "loss at step 328: 0.249883\n",
      "loss at step 329: 0.249883\n",
      "loss at step 330: 0.249882\n",
      "loss at step 331: 0.249882\n",
      "loss at step 332: 0.249882\n",
      "loss at step 333: 0.249882\n",
      "loss at step 334: 0.249882\n",
      "loss at step 335: 0.249882\n",
      "loss at step 336: 0.249882\n",
      "loss at step 337: 0.249882\n",
      "loss at step 338: 0.249882\n",
      "loss at step 339: 0.249882\n",
      "loss at step 340: 0.249882\n",
      "loss at step 341: 0.249882\n",
      "loss at step 342: 0.249882\n",
      "loss at step 343: 0.249882\n",
      "loss at step 344: 0.249882\n",
      "loss at step 345: 0.249882\n",
      "loss at step 346: 0.249881\n",
      "loss at step 347: 0.249881\n",
      "loss at step 348: 0.249881\n",
      "loss at step 349: 0.249881\n",
      "loss at step 350: 0.249881\n",
      "loss at step 351: 0.249881\n",
      "loss at step 352: 0.249881\n",
      "loss at step 353: 0.249881\n",
      "loss at step 354: 0.249881\n",
      "loss at step 355: 0.249881\n",
      "loss at step 356: 0.249881\n",
      "loss at step 357: 0.249881\n",
      "loss at step 358: 0.249881\n",
      "loss at step 359: 0.249881\n",
      "loss at step 360: 0.249881\n",
      "loss at step 361: 0.249881\n",
      "loss at step 362: 0.249881\n",
      "loss at step 363: 0.249881\n",
      "loss at step 364: 0.249881\n",
      "loss at step 365: 0.249880\n",
      "loss at step 366: 0.249880\n",
      "loss at step 367: 0.249880\n",
      "loss at step 368: 0.249880\n",
      "loss at step 369: 0.249880\n",
      "loss at step 370: 0.249880\n",
      "loss at step 371: 0.249880\n",
      "loss at step 372: 0.249880\n",
      "loss at step 373: 0.249880\n",
      "loss at step 374: 0.249880\n",
      "loss at step 375: 0.249880\n",
      "loss at step 376: 0.249880\n",
      "loss at step 377: 0.249880\n",
      "loss at step 378: 0.249880\n",
      "loss at step 379: 0.249880\n",
      "loss at step 380: 0.249880\n",
      "loss at step 381: 0.249880\n",
      "loss at step 382: 0.249880\n",
      "loss at step 383: 0.249880\n",
      "loss at step 384: 0.249880\n",
      "loss at step 385: 0.249880\n",
      "loss at step 386: 0.249879\n",
      "loss at step 387: 0.249880\n",
      "loss at step 388: 0.249879\n",
      "loss at step 389: 0.249879\n",
      "loss at step 390: 0.249879\n",
      "loss at step 391: 0.249879\n",
      "loss at step 392: 0.249879\n",
      "loss at step 393: 0.249879\n",
      "loss at step 394: 0.249879\n",
      "loss at step 395: 0.249879\n",
      "loss at step 396: 0.249879\n",
      "loss at step 397: 0.249879\n",
      "loss at step 398: 0.249879\n",
      "loss at step 399: 0.249879\n"
     ]
    }
   ],
   "source": [
    "# Reset parameters of model\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(input_dim,hidden_dim)))\n",
    "b1 = tf.Variable(tf.random.uniform(shape=(hidden_dim,)))\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim,output_dim)))\n",
    "b2 = tf.Variable(tf.random.uniform(shape=(output_dim,)))\n",
    "\n",
    "losses = [] #Initialize blank list to store loss values\n",
    "learning_rate = 0.1\n",
    "\n",
    "for step in range(400):\n",
    "    loss = training_step(inputs,targets,learning_rate)\n",
    "    losses.append(loss)\n",
    "    print(f\"loss at step {step}: {loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6273b746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(400),losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494bd43",
   "metadata": {},
   "source": [
    "Learning rate `0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c981781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 76.358231\n",
      "loss at step 1: 16241.559570\n",
      "loss at step 2: 211007193088.000000\n",
      "loss at step 3: 3203730638488717738566013232873472.000000\n",
      "loss at step 4:  inf\n",
      "loss at step 5:  nan\n",
      "loss at step 6:  nan\n",
      "loss at step 7:  nan\n",
      "loss at step 8:  nan\n",
      "loss at step 9:  nan\n",
      "loss at step 10:  nan\n",
      "loss at step 11:  nan\n",
      "loss at step 12:  nan\n",
      "loss at step 13:  nan\n",
      "loss at step 14:  nan\n",
      "loss at step 15:  nan\n",
      "loss at step 16:  nan\n",
      "loss at step 17:  nan\n",
      "loss at step 18:  nan\n",
      "loss at step 19:  nan\n",
      "loss at step 20:  nan\n",
      "loss at step 21:  nan\n",
      "loss at step 22:  nan\n",
      "loss at step 23:  nan\n",
      "loss at step 24:  nan\n",
      "loss at step 25:  nan\n",
      "loss at step 26:  nan\n",
      "loss at step 27:  nan\n",
      "loss at step 28:  nan\n",
      "loss at step 29:  nan\n",
      "loss at step 30:  nan\n",
      "loss at step 31:  nan\n",
      "loss at step 32:  nan\n",
      "loss at step 33:  nan\n",
      "loss at step 34:  nan\n",
      "loss at step 35:  nan\n",
      "loss at step 36:  nan\n",
      "loss at step 37:  nan\n",
      "loss at step 38:  nan\n",
      "loss at step 39:  nan\n",
      "loss at step 40:  nan\n",
      "loss at step 41:  nan\n",
      "loss at step 42:  nan\n",
      "loss at step 43:  nan\n",
      "loss at step 44:  nan\n",
      "loss at step 45:  nan\n",
      "loss at step 46:  nan\n",
      "loss at step 47:  nan\n",
      "loss at step 48:  nan\n",
      "loss at step 49:  nan\n",
      "loss at step 50:  nan\n",
      "loss at step 51:  nan\n",
      "loss at step 52:  nan\n",
      "loss at step 53:  nan\n",
      "loss at step 54:  nan\n",
      "loss at step 55:  nan\n",
      "loss at step 56:  nan\n",
      "loss at step 57:  nan\n",
      "loss at step 58:  nan\n",
      "loss at step 59:  nan\n",
      "loss at step 60:  nan\n",
      "loss at step 61:  nan\n",
      "loss at step 62:  nan\n",
      "loss at step 63:  nan\n",
      "loss at step 64:  nan\n",
      "loss at step 65:  nan\n",
      "loss at step 66:  nan\n",
      "loss at step 67:  nan\n",
      "loss at step 68:  nan\n",
      "loss at step 69:  nan\n",
      "loss at step 70:  nan\n",
      "loss at step 71:  nan\n",
      "loss at step 72:  nan\n",
      "loss at step 73:  nan\n",
      "loss at step 74:  nan\n",
      "loss at step 75:  nan\n",
      "loss at step 76:  nan\n",
      "loss at step 77:  nan\n",
      "loss at step 78:  nan\n",
      "loss at step 79:  nan\n",
      "loss at step 80:  nan\n",
      "loss at step 81:  nan\n",
      "loss at step 82:  nan\n",
      "loss at step 83:  nan\n",
      "loss at step 84:  nan\n",
      "loss at step 85:  nan\n",
      "loss at step 86:  nan\n",
      "loss at step 87:  nan\n",
      "loss at step 88:  nan\n",
      "loss at step 89:  nan\n",
      "loss at step 90:  nan\n",
      "loss at step 91:  nan\n",
      "loss at step 92:  nan\n",
      "loss at step 93:  nan\n",
      "loss at step 94:  nan\n",
      "loss at step 95:  nan\n",
      "loss at step 96:  nan\n",
      "loss at step 97:  nan\n",
      "loss at step 98:  nan\n",
      "loss at step 99:  nan\n",
      "loss at step 100:  nan\n",
      "loss at step 101:  nan\n",
      "loss at step 102:  nan\n",
      "loss at step 103:  nan\n",
      "loss at step 104:  nan\n",
      "loss at step 105:  nan\n",
      "loss at step 106:  nan\n",
      "loss at step 107:  nan\n",
      "loss at step 108:  nan\n",
      "loss at step 109:  nan\n",
      "loss at step 110:  nan\n",
      "loss at step 111:  nan\n",
      "loss at step 112:  nan\n",
      "loss at step 113:  nan\n",
      "loss at step 114:  nan\n",
      "loss at step 115:  nan\n",
      "loss at step 116:  nan\n",
      "loss at step 117:  nan\n",
      "loss at step 118:  nan\n",
      "loss at step 119:  nan\n",
      "loss at step 120:  nan\n",
      "loss at step 121:  nan\n",
      "loss at step 122:  nan\n",
      "loss at step 123:  nan\n",
      "loss at step 124:  nan\n",
      "loss at step 125:  nan\n",
      "loss at step 126:  nan\n",
      "loss at step 127:  nan\n",
      "loss at step 128:  nan\n",
      "loss at step 129:  nan\n",
      "loss at step 130:  nan\n",
      "loss at step 131:  nan\n",
      "loss at step 132:  nan\n",
      "loss at step 133:  nan\n",
      "loss at step 134:  nan\n",
      "loss at step 135:  nan\n",
      "loss at step 136:  nan\n",
      "loss at step 137:  nan\n",
      "loss at step 138:  nan\n",
      "loss at step 139:  nan\n",
      "loss at step 140:  nan\n",
      "loss at step 141:  nan\n",
      "loss at step 142:  nan\n",
      "loss at step 143:  nan\n",
      "loss at step 144:  nan\n",
      "loss at step 145:  nan\n",
      "loss at step 146:  nan\n",
      "loss at step 147:  nan\n",
      "loss at step 148:  nan\n",
      "loss at step 149:  nan\n",
      "loss at step 150:  nan\n",
      "loss at step 151:  nan\n",
      "loss at step 152:  nan\n",
      "loss at step 153:  nan\n",
      "loss at step 154:  nan\n",
      "loss at step 155:  nan\n",
      "loss at step 156:  nan\n",
      "loss at step 157:  nan\n",
      "loss at step 158:  nan\n",
      "loss at step 159:  nan\n",
      "loss at step 160:  nan\n",
      "loss at step 161:  nan\n",
      "loss at step 162:  nan\n",
      "loss at step 163:  nan\n",
      "loss at step 164:  nan\n",
      "loss at step 165:  nan\n",
      "loss at step 166:  nan\n",
      "loss at step 167:  nan\n",
      "loss at step 168:  nan\n",
      "loss at step 169:  nan\n",
      "loss at step 170:  nan\n",
      "loss at step 171:  nan\n",
      "loss at step 172:  nan\n",
      "loss at step 173:  nan\n",
      "loss at step 174:  nan\n",
      "loss at step 175:  nan\n",
      "loss at step 176:  nan\n",
      "loss at step 177:  nan\n",
      "loss at step 178:  nan\n",
      "loss at step 179:  nan\n",
      "loss at step 180:  nan\n",
      "loss at step 181:  nan\n",
      "loss at step 182:  nan\n",
      "loss at step 183:  nan\n",
      "loss at step 184:  nan\n",
      "loss at step 185:  nan\n",
      "loss at step 186:  nan\n",
      "loss at step 187:  nan\n",
      "loss at step 188:  nan\n",
      "loss at step 189:  nan\n",
      "loss at step 190:  nan\n",
      "loss at step 191:  nan\n",
      "loss at step 192:  nan\n",
      "loss at step 193:  nan\n",
      "loss at step 194:  nan\n",
      "loss at step 195:  nan\n",
      "loss at step 196:  nan\n",
      "loss at step 197:  nan\n",
      "loss at step 198:  nan\n",
      "loss at step 199:  nan\n",
      "loss at step 200:  nan\n",
      "loss at step 201:  nan\n",
      "loss at step 202:  nan\n",
      "loss at step 203:  nan\n",
      "loss at step 204:  nan\n",
      "loss at step 205:  nan\n",
      "loss at step 206:  nan\n",
      "loss at step 207:  nan\n",
      "loss at step 208:  nan\n",
      "loss at step 209:  nan\n",
      "loss at step 210:  nan\n",
      "loss at step 211:  nan\n",
      "loss at step 212:  nan\n",
      "loss at step 213:  nan\n",
      "loss at step 214:  nan\n",
      "loss at step 215:  nan\n",
      "loss at step 216:  nan\n",
      "loss at step 217:  nan\n",
      "loss at step 218:  nan\n",
      "loss at step 219:  nan\n",
      "loss at step 220:  nan\n",
      "loss at step 221:  nan\n",
      "loss at step 222:  nan\n",
      "loss at step 223:  nan\n",
      "loss at step 224:  nan\n",
      "loss at step 225:  nan\n",
      "loss at step 226:  nan\n",
      "loss at step 227:  nan\n",
      "loss at step 228:  nan\n",
      "loss at step 229:  nan\n",
      "loss at step 230:  nan\n",
      "loss at step 231:  nan\n",
      "loss at step 232:  nan\n",
      "loss at step 233:  nan\n",
      "loss at step 234:  nan\n",
      "loss at step 235:  nan\n",
      "loss at step 236:  nan\n",
      "loss at step 237:  nan\n",
      "loss at step 238:  nan\n",
      "loss at step 239:  nan\n",
      "loss at step 240:  nan\n",
      "loss at step 241:  nan\n",
      "loss at step 242:  nan\n",
      "loss at step 243:  nan\n",
      "loss at step 244:  nan\n",
      "loss at step 245:  nan\n",
      "loss at step 246:  nan\n",
      "loss at step 247:  nan\n",
      "loss at step 248:  nan\n",
      "loss at step 249:  nan\n",
      "loss at step 250:  nan\n",
      "loss at step 251:  nan\n",
      "loss at step 252:  nan\n",
      "loss at step 253:  nan\n",
      "loss at step 254:  nan\n",
      "loss at step 255:  nan\n",
      "loss at step 256:  nan\n",
      "loss at step 257:  nan\n",
      "loss at step 258:  nan\n",
      "loss at step 259:  nan\n",
      "loss at step 260:  nan\n",
      "loss at step 261:  nan\n",
      "loss at step 262:  nan\n",
      "loss at step 263:  nan\n",
      "loss at step 264:  nan\n",
      "loss at step 265:  nan\n",
      "loss at step 266:  nan\n",
      "loss at step 267:  nan\n",
      "loss at step 268:  nan\n",
      "loss at step 269:  nan\n",
      "loss at step 270:  nan\n",
      "loss at step 271:  nan\n",
      "loss at step 272:  nan\n",
      "loss at step 273:  nan\n",
      "loss at step 274:  nan\n",
      "loss at step 275:  nan\n",
      "loss at step 276:  nan\n",
      "loss at step 277:  nan\n",
      "loss at step 278:  nan\n",
      "loss at step 279:  nan\n",
      "loss at step 280:  nan\n",
      "loss at step 281:  nan\n",
      "loss at step 282:  nan\n",
      "loss at step 283:  nan\n",
      "loss at step 284:  nan\n",
      "loss at step 285:  nan\n",
      "loss at step 286:  nan\n",
      "loss at step 287:  nan\n",
      "loss at step 288:  nan\n",
      "loss at step 289:  nan\n",
      "loss at step 290:  nan\n",
      "loss at step 291:  nan\n",
      "loss at step 292:  nan\n",
      "loss at step 293:  nan\n",
      "loss at step 294:  nan\n",
      "loss at step 295:  nan\n",
      "loss at step 296:  nan\n",
      "loss at step 297:  nan\n",
      "loss at step 298:  nan\n",
      "loss at step 299:  nan\n",
      "loss at step 300:  nan\n",
      "loss at step 301:  nan\n",
      "loss at step 302:  nan\n",
      "loss at step 303:  nan\n",
      "loss at step 304:  nan\n",
      "loss at step 305:  nan\n",
      "loss at step 306:  nan\n",
      "loss at step 307:  nan\n",
      "loss at step 308:  nan\n",
      "loss at step 309:  nan\n",
      "loss at step 310:  nan\n",
      "loss at step 311:  nan\n",
      "loss at step 312:  nan\n",
      "loss at step 313:  nan\n",
      "loss at step 314:  nan\n",
      "loss at step 315:  nan\n",
      "loss at step 316:  nan\n",
      "loss at step 317:  nan\n",
      "loss at step 318:  nan\n",
      "loss at step 319:  nan\n",
      "loss at step 320:  nan\n",
      "loss at step 321:  nan\n",
      "loss at step 322:  nan\n",
      "loss at step 323:  nan\n",
      "loss at step 324:  nan\n",
      "loss at step 325:  nan\n",
      "loss at step 326:  nan\n",
      "loss at step 327:  nan\n",
      "loss at step 328:  nan\n",
      "loss at step 329:  nan\n",
      "loss at step 330:  nan\n",
      "loss at step 331:  nan\n",
      "loss at step 332:  nan\n",
      "loss at step 333:  nan\n",
      "loss at step 334:  nan\n",
      "loss at step 335:  nan\n",
      "loss at step 336:  nan\n",
      "loss at step 337:  nan\n",
      "loss at step 338:  nan\n",
      "loss at step 339:  nan\n",
      "loss at step 340:  nan\n",
      "loss at step 341:  nan\n",
      "loss at step 342:  nan\n",
      "loss at step 343:  nan\n",
      "loss at step 344:  nan\n",
      "loss at step 345:  nan\n",
      "loss at step 346:  nan\n",
      "loss at step 347:  nan\n",
      "loss at step 348:  nan\n",
      "loss at step 349:  nan\n",
      "loss at step 350:  nan\n",
      "loss at step 351:  nan\n",
      "loss at step 352:  nan\n",
      "loss at step 353:  nan\n",
      "loss at step 354:  nan\n",
      "loss at step 355:  nan\n",
      "loss at step 356:  nan\n",
      "loss at step 357:  nan\n",
      "loss at step 358:  nan\n",
      "loss at step 359:  nan\n",
      "loss at step 360:  nan\n",
      "loss at step 361:  nan\n",
      "loss at step 362:  nan\n",
      "loss at step 363:  nan\n",
      "loss at step 364:  nan\n",
      "loss at step 365:  nan\n",
      "loss at step 366:  nan\n",
      "loss at step 367:  nan\n",
      "loss at step 368:  nan\n",
      "loss at step 369:  nan\n",
      "loss at step 370:  nan\n",
      "loss at step 371:  nan\n",
      "loss at step 372:  nan\n",
      "loss at step 373:  nan\n",
      "loss at step 374:  nan\n",
      "loss at step 375:  nan\n",
      "loss at step 376:  nan\n",
      "loss at step 377:  nan\n",
      "loss at step 378:  nan\n",
      "loss at step 379:  nan\n",
      "loss at step 380:  nan\n",
      "loss at step 381:  nan\n",
      "loss at step 382:  nan\n",
      "loss at step 383:  nan\n",
      "loss at step 384:  nan\n",
      "loss at step 385:  nan\n",
      "loss at step 386:  nan\n",
      "loss at step 387:  nan\n",
      "loss at step 388:  nan\n",
      "loss at step 389:  nan\n",
      "loss at step 390:  nan\n",
      "loss at step 391:  nan\n",
      "loss at step 392:  nan\n",
      "loss at step 393:  nan\n",
      "loss at step 394:  nan\n",
      "loss at step 395:  nan\n",
      "loss at step 396:  nan\n",
      "loss at step 397:  nan\n",
      "loss at step 398:  nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 399:  nan\n"
     ]
    }
   ],
   "source": [
    "# Reset parameters of model\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(input_dim,hidden_dim)))\n",
    "b1 = tf.Variable(tf.random.uniform(shape=(hidden_dim,)))\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim,output_dim)))\n",
    "b2 = tf.Variable(tf.random.uniform(shape=(output_dim,)))\n",
    "\n",
    "losses = [] #Initialize blank list to store loss values\n",
    "learning_rate = 0.5\n",
    "\n",
    "for step in range(400):\n",
    "    loss = training_step(inputs,targets,learning_rate)\n",
    "    losses.append(loss)\n",
    "    print(f\"loss at step {step}: {loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7b8230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(400),losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d269fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)[3:10][0\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6190963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
